---
title: "Distillation Can Make AI Models Smaller and Cheaper"
date: 2025-09-20T11:00:00+08:00
categories: ["tech"]
tags: ["Science", "Business / Artificial Intelligence", "Quanta Magazine", "artificial intelligence", "science", "Artificial Intelligence"]
summary: "A fundamental technique lets researchers use a big, expensive model to train another model for less."
source_url: "https://www.wired.com/story/how-distillation-makes-ai-models-smaller-and-cheaper/"
---

A fundamental technique lets researchers use a big, expensive model to train another model for less.

---

*来源: [原文链接](https://www.wired.com/story/how-distillation-makes-ai-models-smaller-and-cheaper/)*

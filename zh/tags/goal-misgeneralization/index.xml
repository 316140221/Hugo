<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Goal Misgeneralization on 无关紧要的宇宙</title><link>https://316140221.github.io/Hugo/zh/tags/goal-misgeneralization/</link><description>Recent content in Goal Misgeneralization on 无关紧要的宇宙</description><generator>Hugo</generator><language>zh-CN</language><lastBuildDate>Wed, 13 Aug 2025 20:28:20 +0800</lastBuildDate><atom:link href="https://316140221.github.io/Hugo/zh/tags/goal-misgeneralization/index.xml" rel="self" type="application/rss+xml"/><item><title>Is AI really trying to escape human control and blackmail people?</title><link>https://316140221.github.io/Hugo/zh/en/tech/2025-08-13-is-ai-really-trying-to-escape-human-control-and-bl/</link><pubDate>Wed, 13 Aug 2025 20:28:20 +0800</pubDate><guid>https://316140221.github.io/Hugo/zh/en/tech/2025-08-13-is-ai-really-trying-to-escape-human-control-and-bl/</guid><description>Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.</description></item></channel></rss>